{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Function to read YOLO label files\n",
    "def read_yolo_label(label_path):\n",
    "    boxes = []\n",
    "    try:\n",
    "        with open(label_path, 'r') as file:\n",
    "            for line in file.readlines():\n",
    "                values = line.strip().split()\n",
    "                class_id = int(values[0])  # Class ID\n",
    "                x_center, y_center, width, height = map(float, values[1:])  # YOLO format\n",
    "                boxes.append([class_id, x_center, y_center, width, height])\n",
    "    except FileNotFoundError:\n",
    "        # Handle missing label files gracefully\n",
    "        print(f\"Label file not found: {label_path}. Returning empty boxes.\")\n",
    "    return torch.tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((448, 448)),  # Resize all images to the same size\n",
    "    transforms.ToTensor(),  # Convert to PyTorch tensors\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize RGB channels\n",
    "])\n",
    "\n",
    "class DriveYoloDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.label_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Populate image and label paths\n",
    "        for class_name in os.listdir(root_dir):  # Iterate through \"open\", \"closed\"\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_dir):\n",
    "                label = 0 if class_name.lower() == \"open\" else 1  # Assign numeric labels\n",
    "                for file_name in os.listdir(class_dir):\n",
    "                    if file_name.endswith(\".jpg\"):  # Look for image files\n",
    "                        label_path = os.path.splitext(os.path.join(class_dir, file_name))[0] + \".txt\"\n",
    "                        if os.path.exists(label_path):  # Check if label file exists\n",
    "                            self.image_paths.append(os.path.join(class_dir, file_name))\n",
    "                            self.label_paths.append(label_path)\n",
    "                            self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Apply transforms to the image\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # Load bounding box labels\n",
    "        label_path = self.label_paths[idx]\n",
    "        boxes = read_yolo_label(label_path)\n",
    "\n",
    "        # Fetch the open/closed label\n",
    "        eye_state_label = self.labels[idx]\n",
    "\n",
    "        return img, boxes, eye_state_label\n",
    "\n",
    "batch_size = 1\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle batches with varying numbers of bounding boxes.\n",
    "    \"\"\"\n",
    "    images, boxes, labels = zip(*batch)\n",
    "    images = torch.stack(images)  # Stack images into a single tensor\n",
    "    return images, boxes, torch.tensor(labels)\n",
    "\n",
    "drive_train_dir = './drive_image_data/train'\n",
    "train_dataset = DriveYoloDataset(root_dir=drive_train_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set device to MPS if available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "class yoloV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=3)\n",
    "        self.maxPool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.maxPool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(192, 128, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv6 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.maxPool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv7 = nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv8 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv9 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv10 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.conv11 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv12 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv13 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv14 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.conv15 = nn.Conv2d(512, 512, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.conv16 = nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.maxPool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv17 = nn.Conv2d(1024, 512, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv18 = nn.Conv2d(512, 512, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.conv19 = nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv20 = nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.conv21 = nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv22 = nn.Conv2d(1024, 1024, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv23 = nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv24 = nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(200704, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 7 * 7 * (1 + 2 * 5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = self.maxPool1(x)\n",
    "\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        x = self.maxPool2(x)\n",
    "\n",
    "        x = F.leaky_relu(self.conv3(x))           \n",
    "        x = F.leaky_relu(self.conv4(x))           \n",
    "        x = F.leaky_relu(self.conv5(x))           \n",
    "        x = F.leaky_relu(self.conv6(x))           \n",
    "        x = self.maxPool3(x)                      \n",
    "\n",
    "        x = F.leaky_relu(self.conv7(x))           \n",
    "        x = F.leaky_relu(self.conv8(x))           \n",
    "        x = F.leaky_relu(self.conv9(x))           \n",
    "        x = F.leaky_relu(self.conv10(x))          \n",
    "        x = F.leaky_relu(self.conv11(x))          \n",
    "        x = F.leaky_relu(self.conv12(x))         \n",
    "        x = F.leaky_relu(self.conv13(x))          \n",
    "        x = F.leaky_relu(self.conv14(x))          \n",
    "\n",
    "        x = F.leaky_relu(self.conv15(x))          \n",
    "        x = F.leaky_relu(self.conv16(x))          \n",
    "        x = self.maxPool4(x)                      \n",
    "\n",
    "        x = F.leaky_relu(self.conv17(x))         \n",
    "        x = F.leaky_relu(self.conv18(x))         \n",
    "        x = F.leaky_relu(self.conv19(x))          \n",
    "        x = F.leaky_relu(self.conv20(x))          \n",
    "        x = F.leaky_relu(self.conv21(x))          \n",
    "        x = F.leaky_relu(self.conv22(x))\n",
    "        x = F.leaky_relu(self.conv23(x))\n",
    "        x = F.leaky_relu(self.conv24(x))                  \n",
    "\n",
    "        # Flatten for fully connected layers\n",
    "        x = x.view(x.size(0), -1)                 \n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.leaky_relu(self.fc1(x))            \n",
    "        x = F.leaky_relu(self.fc2(x))            \n",
    "        x = self.fc3(x)                           \n",
    "        \n",
    "        # Reshape the final output for YOLO\n",
    "        x = x.view(-1, 7, 7, (1 + 2 * 5))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
    "    \"\"\"\n",
    "    Calculates Intersection over Union (IoU) between two sets of bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        boxes_preds (tensor): Predicted bounding boxes of shape (N, 4).\n",
    "                              Format depends on `box_format`.\n",
    "        boxes_labels (tensor): Ground truth bounding boxes of shape (N, 4).\n",
    "                               Format depends on `box_format`.\n",
    "        box_format (str): \"midpoint\" or \"corners\".\n",
    "                          - \"midpoint\": (x_center, y_center, width, height)\n",
    "                          - \"corners\": (x_min, y_min, x_max, y_max)\n",
    "\n",
    "    Returns:\n",
    "        tensor: IoU for each bounding box, shape (N).\n",
    "    \"\"\"\n",
    "    if box_format == \"midpoint\":\n",
    "        # Convert from midpoint to corners\n",
    "        box1_x1 = boxes_preds[..., 0] - boxes_preds[..., 2] / 2\n",
    "        box1_y1 = boxes_preds[..., 1] - boxes_preds[..., 3] / 2\n",
    "        box1_x2 = boxes_preds[..., 0] + boxes_preds[..., 2] / 2\n",
    "        box1_y2 = boxes_preds[..., 1] + boxes_preds[..., 3] / 2\n",
    "        box2_x1 = boxes_labels[..., 0] - boxes_labels[..., 2] / 2\n",
    "        box2_y1 = boxes_labels[..., 1] - boxes_labels[..., 3] / 2\n",
    "        box2_x2 = boxes_labels[..., 0] + boxes_labels[..., 2] / 2\n",
    "        box2_y2 = boxes_labels[..., 1] + boxes_labels[..., 3] / 2\n",
    "    elif box_format == \"corners\":\n",
    "        # If already in corner format\n",
    "        box1_x1, box1_y1, box1_x2, box1_y2 = boxes_preds[..., 0], boxes_preds[..., 1], boxes_preds[..., 2], boxes_preds[..., 3]\n",
    "        box2_x1, box2_y1, box2_x2, box2_y2 = boxes_labels[..., 0], boxes_labels[..., 1], boxes_labels[..., 2], boxes_labels[..., 3]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid box_format. Choose 'midpoint' or 'corners'.\")\n",
    "\n",
    "    # Intersection coordinates\n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "    # Calculate intersection area\n",
    "    intersection = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)\n",
    "\n",
    "    # Calculate areas of both boxes\n",
    "    box1_area = (box1_x2 - box1_x1) * (box1_y2 - box1_y1)\n",
    "    box2_area = (box2_x2 - box2_x1) * (box2_y2 - box2_y1)\n",
    "\n",
    "    # Calculate union area\n",
    "    union = box1_area + box2_area - intersection + 1e-6  # Add epsilon to avoid division by zero\n",
    "\n",
    "    # IoU\n",
    "    iou = intersection / union\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 106.60862731933594\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self, S=7, B=2, C=1, lambda_coord=5, lambda_noobj=0.5):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.S = S  # Grid size (SxS)\n",
    "        self.B = B  # Number of bounding boxes per grid cell\n",
    "        self.C = C  # Number of classes (binary: 1)\n",
    "        self.lambda_coord = lambda_coord  # Weight for localization loss\n",
    "        self.lambda_noobj = lambda_noobj  # Weight for no-object confidence loss\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        \"\"\"\n",
    "        Computes the YOLOv1 loss for binary classification.\n",
    "\n",
    "        Args:\n",
    "            predictions: (batch_size, S * S * (C + B * 5)) - predicted output\n",
    "            target: (batch_size, S, S, (C + B * 5)) - ground truth\n",
    "\n",
    "        Returns:\n",
    "            Total loss as a scalar\n",
    "        \"\"\"\n",
    "        # Reshape predictions into (batch_size, S, S, C + B * 5)\n",
    "        predictions = predictions.view(-1, self.S, self.S, self.C + self.B * 5)\n",
    "\n",
    "        # Compute IoUs for each bounding box\n",
    "        iou_b1 = self.compute_iou(predictions[..., 1:5], target[..., 1:5])\n",
    "        iou_b2 = self.compute_iou(predictions[..., 6:10], target[..., 6:10])\n",
    "        ious = torch.stack((iou_b1, iou_b2), dim=0)  # Shape: (2, batch_size, S, S)\n",
    "\n",
    "        # Determine the best box for each grid cell\n",
    "        iou_maxes, bestbox = torch.max(ious, dim=0)  # Shape: (batch_size, S, S)\n",
    "        exists_box = target[..., 0].unsqueeze(-1)  # Ground truth object mask\n",
    "\n",
    "        # Box Predictions\n",
    "        box_predictions = (\n",
    "            bestbox.unsqueeze(-1) * predictions[..., 6:10]\n",
    "            + (1 - bestbox.unsqueeze(-1)) * predictions[..., 1:5]\n",
    "        )\n",
    "        box_predictions = torch.cat(\n",
    "            [\n",
    "                box_predictions[..., :2],\n",
    "                torch.sqrt(torch.clamp(box_predictions[..., 2:4], min=1e-6)),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "        # Box Targets\n",
    "        box_targets = torch.cat(\n",
    "            [\n",
    "                target[..., 1:3],\n",
    "                torch.sqrt(torch.clamp(target[..., 3:5], min=1e-6)),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "        # Compute box loss\n",
    "        box_loss = self.lambda_coord * self.mse(\n",
    "            (exists_box * box_predictions).view(-1, 4),\n",
    "            (exists_box * box_targets).view(-1, 4),\n",
    "        )\n",
    "\n",
    "        # Confidence Loss (Object)\n",
    "        pred_conf = (\n",
    "            bestbox * predictions[..., 5] + (1 - bestbox) * predictions[..., 0]\n",
    "        )\n",
    "        object_loss = self.mse(\n",
    "            (exists_box.squeeze(-1) * pred_conf).view(-1),\n",
    "            (exists_box.squeeze(-1) * target[..., 0]).view(-1),\n",
    "        )\n",
    "\n",
    "        # Confidence Loss (No Object)\n",
    "        no_object_loss = self.lambda_noobj * self.mse(\n",
    "            ((1 - exists_box.squeeze(-1)) * predictions[..., 0]).view(-1),\n",
    "            ((1 - exists_box.squeeze(-1)) * target[..., 0]).view(-1),\n",
    "        ) + self.lambda_noobj * self.mse(\n",
    "            ((1 - exists_box.squeeze(-1)) * predictions[..., 5]).view(-1),\n",
    "            ((1 - exists_box.squeeze(-1)) * target[..., 0]).view(-1),\n",
    "        )\n",
    "\n",
    "        # Class Loss\n",
    "        class_loss = self.mse(\n",
    "            (exists_box * predictions[..., :self.C]).view(-1, self.C),\n",
    "            (exists_box * target[..., :self.C]).view(-1, self.C),\n",
    "        )\n",
    "\n",
    "        # Total Loss\n",
    "        total_loss = box_loss + object_loss + no_object_loss + class_loss\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_iou(boxes1, boxes2):\n",
    "        \"\"\"\n",
    "        Compute the Intersection over Union (IoU) for predicted and target boxes.\n",
    "        \n",
    "        Args:\n",
    "        boxes1 (torch.Tensor): Predicted boxes, shape (batch_size, S, S, 4)\n",
    "        boxes2 (torch.Tensor): Target boxes, shape (batch_size, S, S, 4)\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: IoU scores, shape (batch_size, S, S)\n",
    "        \"\"\"\n",
    "        # Unpack coordinates\n",
    "        b1_x1 = boxes1[..., 0] - boxes1[..., 2] / 2\n",
    "        b1_y1 = boxes1[..., 1] - boxes1[..., 3] / 2\n",
    "        b1_x2 = boxes1[..., 0] + boxes1[..., 2] / 2\n",
    "        b1_y2 = boxes1[..., 1] + boxes1[..., 3] / 2\n",
    "\n",
    "        b2_x1 = boxes2[..., 0] - boxes2[..., 2] / 2\n",
    "        b2_y1 = boxes2[..., 1] - boxes2[..., 3] / 2\n",
    "        b2_x2 = boxes2[..., 0] + boxes2[..., 2] / 2\n",
    "        b2_y2 = boxes2[..., 1] + boxes2[..., 3] / 2\n",
    "\n",
    "        # Intersection coordinates\n",
    "        inter_x1 = torch.max(b1_x1, b2_x1)\n",
    "        inter_y1 = torch.max(b1_y1, b2_y1)\n",
    "        inter_x2 = torch.min(b1_x2, b2_x2)\n",
    "        inter_y2 = torch.min(b1_y2, b2_y2)\n",
    "\n",
    "        # Intersection area\n",
    "        inter_area = torch.clamp(inter_x2 - inter_x1, min=0) * torch.clamp(\n",
    "            inter_y2 - inter_y1, min=0\n",
    "        )\n",
    "\n",
    "        # Union area\n",
    "        b1_area = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)\n",
    "        b2_area = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)\n",
    "        union_area = b1_area + b2_area - inter_area\n",
    "\n",
    "        return inter_area / torch.clamp(union_area, min=1e-6)\n",
    "\n",
    "\n",
    "# Test the corrected code\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulated test case\n",
    "    batch_size = 2\n",
    "    S = 7\n",
    "    B = 2\n",
    "    C = 1\n",
    "\n",
    "    predictions = torch.rand(batch_size, S * S * (C + B * 5), requires_grad=True)\n",
    "    target = torch.rand(batch_size, S, S, C + B * 5)\n",
    "\n",
    "    criterion = YoloLoss(S=S, B=B, C=C)\n",
    "    loss = criterion(predictions, target)\n",
    "    print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 68\u001b[0m\n\u001b[1;32m     64\u001b[0m         train_losses\u001b[38;5;241m.\u001b[39mappend(total_loss)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Train the Model\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m train_model(model, train_loader, criterion, optimizer)\n",
      "Cell \u001b[0;32mIn[13], line 45\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     43\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     44\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 45\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Compute IoU-based accuracy\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(images)):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = yoloV1().to(device)\n",
    "\n",
    "\n",
    "batch_size =1\n",
    "\n",
    "drive_train_dir = './drive_image_data/train'\n",
    "train_dataset = DriveYoloDataset(root_dir=drive_train_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "criterion = YoloLoss()\n",
    "train_losses = []\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, loader, criterion, optimizer, num_epochs=20):\n",
    "    model.train()\n",
    "    S, B, C = 7, 2, 1  # Grid size, bounding boxes, and classes\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        for images, boxes, labels in loader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Initialize targets\n",
    "            targets = torch.zeros((images.size(0), S, S, C + B * 5)).to(device)  # Updated for clarity\n",
    "            for idx in range(len(boxes)):\n",
    "                for box in boxes[idx]:  # box: (class, x, y, w, h)\n",
    "                    grid_x, grid_y = int(box[1] * S), int(box[2] * S)\n",
    "                    targets[idx, grid_y, grid_x, 0] = 1  # Objectness\n",
    "                    targets[idx, grid_y, grid_x, 1:6] = torch.tensor([1.0, *box[1:]])  # Add confidence\n",
    "                    targets[idx, grid_y, grid_x, 6:] = torch.tensor([1.0, *box[1:]])  # For the second box\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute IoU-based accuracy\n",
    "            for i in range(len(images)):\n",
    "                pred_boxes = outputs[i, ..., C + 1:C + 5].detach()\n",
    "                true_boxes = targets[i, ..., C + 1:C + 5].detach()\n",
    "                pred_classes = outputs[i, ..., :C] > 0.5  # Binary classification for C=1\n",
    "                true_classes = targets[i, ..., 0]  # Objectness as binary\n",
    "\n",
    "                ious = intersection_over_union(pred_boxes, true_boxes)\n",
    "                correct_iou = (ious > 0.5).sum().item()\n",
    "                correct_predictions += correct_iou\n",
    "\n",
    "                total_predictions += true_classes.sum().item()  # Count valid true boxes\n",
    "\n",
    "        # Compute accuracy percentage\n",
    "        accuracy = (correct_predictions / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "        train_losses.append(total_loss)\n",
    "\n",
    "\n",
    "# Train the Model\n",
    "train_model(model, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(train_losses)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolo model loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "plt.plot(train_losses)\n",
    "plt.title('yolo model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.xticks(np.arange(0,20, step=1))\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
